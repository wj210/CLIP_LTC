{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Background Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home2/weijie210/miniconda3/envs/clip_ltc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/export/home2/weijie210/miniconda3/envs/clip_ltc/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x155550b6a890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from torch.nn import functional as F\n",
    "import os.path\n",
    "from collections import defaultdict\n",
    "from utils.misc import ablate,seed_all\n",
    "from utils.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.eval_utils import *\n",
    "from dataset.cub_classes import place_classes\n",
    "from dataset.imagenet_classes import imagenet_classes\n",
    "from compute_prs import model_pretrained_dict\n",
    "from utils.debias_prompt import debias_text_prompt\n",
    "from utils.roboshot import *\n",
    "import random\n",
    "import argparse\n",
    "from main_bg import *\n",
    "from tqdm import tqdm\n",
    "autocast = torch.cuda.amp.autocast\n",
    "seed_all()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home2/weijie210/clip_text_span/utils/factory.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "# model = \"ViT-H-14\"\n",
    "model = \"ViT-B-16\"\n",
    "dataset = \"binary_waterbirds\"\n",
    "input_dir = './output_dir'\n",
    "\n",
    "clip_model,_,preprocess = create_model_and_transforms(model,pretrained=model_pretrained_dict[model])\n",
    "clip_model.to('cuda')\n",
    "clip_model.eval()\n",
    "tokenizer = get_tokenizer(model)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 0 count: 3498\n",
      "Group 1 count: 184\n",
      "Group 2 count: 56\n",
      "Group 3 count: 1057\n"
     ]
    }
   ],
   "source": [
    "from dataset.binary_waterbirds import BinaryWaterbirds\n",
    "root = '../imagenet'\n",
    "waterbirds_ds = BinaryWaterbirds(root=f'{root}/waterbirds/waterbird_complete95_forest2water2', split='train', transform=preprocess,return_filename=False,return_spurious=True) # get spurious\n",
    "\n",
    "# See if model can do well while predicting spurious labels\n",
    "S_labels = waterbirds_ds.targets_spurious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wg(gp,name_):\n",
    "    if 'avg' in gp:\n",
    "        avg_acc = gp.pop('avg')*100\n",
    "    else:\n",
    "        avg_acc = np.mean(list(gp.values()))*100\n",
    "    wg = sorted(gp.values())[0]*100\n",
    "    gap = avg_acc - wg\n",
    "    print(f'{name_} WG/AVG/Gap: {wg:.1f} & {avg_acc:.1f} & {gap:.1f}')\n",
    "    return wg,avg_acc,gap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training attn/mlp states and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "attns,mlps,classifier,labels,grp_labels,grp_counts,place_label = load_bg_ds(dataset,model,input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_attns,val_mlps,val_classifier,val_labels,val_grp_labels,val_grp_counts,val_place_label = load_bg_ds(dataset,model,input_dir,val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred the Spurious label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3320426/4094097451.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on spurious labels: 0.91\n"
     ]
    }
   ],
   "source": [
    "S_template = ['A photo of a bird on land background.','A photo of a bird on water background.']\n",
    "# S_template = ['A photo of a bird on land surroundings.','A photo of a bird on water surroundings.']\n",
    "\n",
    "with torch.no_grad(), autocast():\n",
    "    texts = tokenizer(S_template).to(device)\n",
    "    S_embeddings = clip_model.encode_text(texts)\n",
    "    S_embeddings = torch.nn.functional.normalize(S_embeddings, dim=-1).T.float().detach().cpu() \n",
    "\n",
    "baseline_logits = ((attns.sum(axis = (1,2)) + mlps.sum(axis = 1)) @ S_embeddings).float()\n",
    "S_pred = torch.argmax(baseline_logits,dim=1)\n",
    "\n",
    "S_acc = (S_pred == S_labels).float().mean().item()\n",
    "print(f'Accuracy on spurious labels: {S_acc:.2f}')\n",
    "\n",
    "pred_grp_labels = (labels* 2 + S_pred).long()\n",
    "pred_grp_counts = torch.bincount(pred_grp_labels,minlength = torch.max(pred_grp_labels)+1)\n",
    "pred_grp_counts = {k:v for k,v in enumerate(pred_grp_counts)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val baseline Group 0 acc: 0.944, count: 3414\n",
      "val baseline Group 1 acc: 0.489, count: 268\n",
      "val baseline Group 2 acc: 0.475, count: 240\n",
      "val baseline Group 3 acc: 0.908, count: 873\n",
      "val baseline performance : 0.889\n",
      "Val baseline performance: 88.9\n",
      "Bias heads: [(10, 10), (11, 3), (11, 6)]\n",
      "Impt heads: [(11, 5)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# val_gp = get_group_perf(attns,mlps,classifier,labels,grp_labels,grp_counts,type_ = 'val baseline')\n",
    "val_gp = get_group_perf(attns,mlps,classifier,labels,pred_grp_labels,pred_grp_counts,type_ = 'val baseline') # use pred S\n",
    "\n",
    "\n",
    "correct_pos,wrong_pos,baseline_pred,other_labels = get_correct_wrong(attns,mlps,classifier,labels)\n",
    "biased_correct,biased_wrong = get_biased_pos(wb_biased_grps,pred_grp_labels,correct_pos,wrong_pos) \n",
    "# biased_correct,biased_wrong = get_biased_pos(wb_biased_grps,grp_labels,correct_pos,wrong_pos) \n",
    "\n",
    "gn_correct = get_counts(attns,mlps,baseline_pred,classifier,biased_correct,other_labels)\n",
    "gn_wrong = get_counts(attns,mlps,baseline_pred,classifier,biased_wrong,other_labels)\n",
    "\n",
    "\n",
    "## Finding the cls bis heads\n",
    "bias_head_pos = get_impt_heads(gn_correct,gn_wrong,0.)\n",
    "impt_head_pos = get_impt_heads(gn_wrong,gn_correct,0.)\n",
    "print ('Bias heads:',bias_head_pos)\n",
    "print ('Impt heads:',impt_head_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap({'correct':gn_correct.numpy(),'wrong':gn_wrong.numpy()},heads_from = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test\n",
    "test_attns,test_mlps,classifier,test_labels,test_grp_labels,test_grp_counts,test_place_label = load_bg_ds(dataset,model,input_dir,test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_activations = {}\n",
    "all_activations['Baseline'] = test_attns.sum(axis = (1,2)) + test_mlps.sum(axis = 1)\n",
    "all_activations['ortho'] = test_attns.sum(axis = (1,2)) + test_mlps.sum(axis = 1)\n",
    "\n",
    "ablated_test_attns = ablate(test_attns,bias_head_pos,type ='mean')\n",
    "debiased_classifier = debias_text_prompt(clip_model,tokenizer,'cuda',classifier.T,dataset) # Debias\n",
    "\n",
    "visual_proj = test_attns.sum(dim=(1,2)) + test_mlps.sum(dim=1)\n",
    "\n",
    "roboshot_r = defaultdict(list)\n",
    "ltc_r = defaultdict(list)\n",
    "\n",
    "for i in range(1):\n",
    "    rs_proj = rs_ortho(visual_proj,clip_model,tokenizer,dataset,accept_no=i) # roboshot\n",
    "    all_activations['Roboshot'] = rs_proj\n",
    "    rs_ablate_proj = ablated_test_attns.clone() # ablate bias head and roboshot\n",
    "    for (l,h) in impt_head_pos:\n",
    "        rs_ablate_proj[:,l,h] = rs_ortho(rs_ablate_proj[:,l,h],clip_model,tokenizer,dataset,mode = 'accept',accept_no=i)\n",
    "    all_activations['LTC'] = (rs_ablate_proj.sum(dim=(1,2)) + test_mlps.sum(dim=1))\n",
    "\n",
    "    for k,v in all_activations.items():\n",
    "        class_emb = classifier if k.lower() not in ['ortho'] else debiased_classifier\n",
    "        gp=get_group_perf(v,None,class_emb,test_labels.to('cpu'),test_grp_labels,test_grp_counts,type_ = k,logger=None,print_=False)\n",
    "        wg,avg,gap = print_wg(gp,k)\n",
    "        if k == 'Roboshot':\n",
    "            roboshot_r['wg'].append(wg)\n",
    "            roboshot_r['avg'].append(avg)\n",
    "            roboshot_r['gap'].append(gap)\n",
    "        elif k == 'LTC':\n",
    "            ltc_r['wg'].append(wg)\n",
    "            ltc_r['avg'].append(avg)\n",
    "            ltc_r['gap'].append(gap)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "hidden_dim_map = {'ViT-B-16':128, 'ViT-L-14':256, 'ViT-H-14':256}\n",
    "    \n",
    "class Probe(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    2-layer bottleneck MLP of the form\n",
    "        u  ->  ReLU(W1ᵀ u)  ->  W2ᵀ ->  scalar logit\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim = hidden_dim_map[model], *,\n",
    "                 dtype: torch.dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, hidden_dim, bias=True, dtype=dtype),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, 1, bias=True, dtype=dtype),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x).squeeze(-1)        # ⇒ (batch,)\n",
    "\n",
    "def train_probe(\n",
    "    acts,\n",
    "    labels,\n",
    "    lr=1e-2,\n",
    "    epochs=1,\n",
    "    seed=42,\n",
    "    val_acts=None,\n",
    "    val_labels = None,\n",
    "    val_grp_labels = None,\n",
    "    bz = 64,\n",
    "    weight_decay=1e-4,\n",
    "    metric ='worst',\n",
    "    probe = None,\n",
    "\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.set_grad_enabled(True)\n",
    "    d_probe = acts.shape[-1]\n",
    "    if probe is None:\n",
    "        probe = Probe(d_probe).to(device)\n",
    "\n",
    "    decay, no_decay = [], []\n",
    "    for name, p in probe.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        (decay if p.ndim > 1 else no_decay).append(p)\n",
    "    param_groups = [\n",
    "        {\"params\": decay,     \"weight_decay\": weight_decay},\n",
    "        {\"params\": no_decay,  \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(param_groups, lr=lr)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    best_val_metric = 0 if metric in ['worst','avg'] else 1\n",
    "    best_probe = deepcopy(probe)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        ## Training\n",
    "        for batch_id in range(0, len(acts), bz):\n",
    "            batch_acts = acts[batch_id:batch_id + bz]\n",
    "            batch_labels = labels[batch_id:batch_id + bz]\n",
    "            optimizer.zero_grad()\n",
    "            logits = probe(batch_acts)\n",
    "            \n",
    "            loss = criterion(logits, batch_labels.to(logits))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_epoch_acc = defaultdict(list)\n",
    "        for batch_id in range(0, len(val_acts), bz):\n",
    "            val_batch_acts = val_acts[batch_id:batch_id + bz]\n",
    "            val_batch_labels = val_labels[batch_id:batch_id + bz]\n",
    "            with torch.no_grad():\n",
    "                logits_val = probe(val_batch_acts)\n",
    "                pred_val = (logits_val > 0.0).long()\n",
    "                val_acc = (pred_val == val_batch_labels.to(logits_val)).float().tolist()\n",
    "            for gp,a in zip(val_grp_labels,val_acc):\n",
    "                val_epoch_acc[gp.item()].append(a)\n",
    "        for k,v in val_epoch_acc.items():\n",
    "            val_epoch_acc[k] = np.mean(v)\n",
    "        \n",
    "        # get worst group acc\n",
    "        wg = sorted(val_epoch_acc.items(),key = lambda x: x[1])[0][1]\n",
    "        avg = np.mean(list(val_epoch_acc.values()))\n",
    "        gap = avg - wg\n",
    "\n",
    "        val_metric = {'worst':wg,'avg':avg,'gap':gap}[metric]\n",
    "        if metric in ['worst','avg']:\n",
    "            if val_metric > best_val_metric:\n",
    "                best_val_metric = val_metric\n",
    "                best_probe = deepcopy(probe)\n",
    "        else:\n",
    "            if val_metric < best_val_metric:\n",
    "                best_val_metric = val_metric\n",
    "                best_probe = deepcopy(probe)\n",
    "            \n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    return best_probe, best_val_metric\n",
    "    \n",
    "    \n",
    "@torch.no_grad()\n",
    "def test_probe(\n",
    "    probe,\n",
    "    acts,\n",
    "    labels,\n",
    "    grp_labels= None,\n",
    "    bz = 64,\n",
    "):\n",
    "    grp_acc = defaultdict(list)\n",
    "    mean_acc = []\n",
    "    for batch_id in range(0, len(acts), bz):\n",
    "        batch_acts = acts[batch_id:batch_id + bz]\n",
    "        batch_labels = labels[batch_id:batch_id + bz]\n",
    "        batch_grp_labels = grp_labels[batch_id:batch_id + bz]\n",
    "\n",
    "        logits = probe(batch_acts)\n",
    "        preds = (logits > 0.0).long()\n",
    "        acc = (preds == batch_labels.to(preds)).float()\n",
    "        mean_acc.append(acc.mean().item())\n",
    "        for g,a in zip(batch_grp_labels,acc):\n",
    "            grp_acc[g.item()].append(a.item())\n",
    "            \n",
    "    for k,v in grp_acc.items():\n",
    "        grp_acc[k] = np.mean(v)\n",
    "    return mean_acc, grp_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_group_samples(grp_labels,acts,labels,worst_grp,upsample):\n",
    "    upsampled_acts = []\n",
    "    upsampled_labels = []\n",
    "    for grp_label,act,label in zip(grp_labels,acts,labels):\n",
    "        if grp_label.item() in worst_grp:\n",
    "            upsampled_acts.append(act.unsqueeze(0).repeat(upsample,1))\n",
    "            upsampled_labels.append(label.unsqueeze(0).repeat(upsample))\n",
    "        else:\n",
    "            upsampled_acts.append(act.unsqueeze(0))\n",
    "            upsampled_labels.append(label.unsqueeze(0))\n",
    "    upsampled_acts = torch.cat(upsampled_acts,dim=0)\n",
    "    upsampled_labels = torch.cat(upsampled_labels,dim=0)\n",
    "    perm = torch.randperm(len(upsampled_acts))\n",
    "    return upsampled_acts[perm], upsampled_labels[perm]\n",
    "\n",
    "def get_ltc_repr(attns,mlps,bias_head=None,impt_head=None):\n",
    "    cloned_attns = attns.clone()\n",
    "    if bias_head is not None:\n",
    "        cloned_attns = ablate(cloned_attns,bias_head)\n",
    "    if impt_head is not None:\n",
    "        for (l,h) in impt_head:\n",
    "            cloned_attns[:,l,h] = rs_ortho(cloned_attns[:,l,h],clip_model,tokenizer,dataset,mode = 'accept')\n",
    "    return (cloned_attns.sum(axis = (1,2)) + mlps.sum(axis = 1)).to(device)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_repr = (attns.sum(axis = (1,2)) + mlps.sum(axis = 1)).to(device)\n",
    "train_labels = labels.to(device)\n",
    "\n",
    "val_img_repr = (val_attns.sum(axis = (1,2)) + val_mlps.sum(axis = 1)).to(device)\n",
    "val_labels = val_labels.to(device)\n",
    "\n",
    "test_img_repr = (test_attns.sum(axis = (1,2)) + test_mlps.sum(axis = 1)).to(device)\n",
    "test_labels = test_labels.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ERM probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP\n",
    "lr = 1e-2\n",
    "wd = 1e-4\n",
    "G_N = [1,2]\n",
    "bz = 64\n",
    "total_epochs = 100\n",
    "upsample = 90 if model == 'ViT-H-14' else 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERM WG/AVG/Gap: 35.2 & 78.5 & 43.3\n"
     ]
    }
   ],
   "source": [
    "erm_probe,_, = train_probe(train_img_repr,train_labels,epochs = total_epochs,lr = lr,weight_decay = wd,val_acts = val_img_repr,val_labels = val_labels,val_grp_labels=val_grp_labels,bz = bz)\n",
    "erm_mean_acc,erm_grp_acc = test_probe(erm_probe,test_img_repr,test_labels,test_grp_labels,bz = bz)\n",
    "_,_,_ = print_wg(erm_grp_acc,f'ERM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JTT WG/AVG/Gap: 72.3 & 87.2 & 14.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "erm_train_repr,erm_train_labels = repeat_group_samples(pred_grp_labels,train_img_repr,train_labels,G_N,upsample=upsample)\n",
    "\n",
    "erm_probe,erm_val_acc, = train_probe(erm_train_repr,erm_train_labels,epochs = total_epochs,lr = lr,weight_decay = wd,val_acts = val_img_repr,val_labels = val_labels,val_grp_labels=val_grp_labels,bz = bz)\n",
    "\n",
    "erm_mean_acc,erm_grp_acc = test_probe(erm_probe,test_img_repr,test_labels,test_grp_labels,bz = bz)\n",
    "_,_,_ = print_wg(erm_grp_acc,f'JTT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JTT + LTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JTT + LTC WG/AVG/Gap: 86.4 & 91.2 & 4.8\n"
     ]
    }
   ],
   "source": [
    "ltc_test_repr = get_ltc_repr(test_attns,test_mlps,bias_head = bias_head_pos,impt_head = impt_head_pos)\n",
    "erm_mean_acc,erm_grp_acc = test_probe(erm_probe,ltc_test_repr,test_labels,test_grp_labels,bz = 64,)\n",
    "_,_,_ = print_wg(erm_grp_acc,f'JTT + LTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
